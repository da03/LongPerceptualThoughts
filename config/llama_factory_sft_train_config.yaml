### model
model_name_or_path: /root/computer/computer/train_dataset_encoded4/LongPerceptualThoughts/data_gen/Qwen2.5-VL-Instruct-7B
template: qwen2_vl
image_max_pixels: 262144 # 512*512

### method
stage: sft
do_train: true
finetuning_type: full
freeze_vision_tower: True
freeze_multi_modal_projector: True
deepspeed: config/deepspeed/ds_z3_config.json

### dataset
dataset_dir: third_party_packages/LLaMA-Factory/data
dataset: long_perceptual_thoughts/sft_docci_all_extended_cots
cutoff_len: 2048
overwrite_cache: true
preprocessing_num_workers: 4
torch_empty_cache_steps: 1
preprocessing_batch_size: 500
tokenized_path: 

### output
output_dir: outputs/example_sft_all_extended_cots
logging_steps: 5
save_steps: 50
save_total_limit: null
load_best_model_at_end: false
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 0.00001
num_train_epochs: 1
max_samples: 10000000000.0
lr_scheduler_type: cosine
warmup_steps: 5
bf16: true
ddp_timeout: 180000000
eval_on_start: false
gradient_checkpointing: true

### WandB
report_to: wandb
run_name: example_sft_all_extended_cots

flash_attn: fa2
enable_liger_kernel: true
